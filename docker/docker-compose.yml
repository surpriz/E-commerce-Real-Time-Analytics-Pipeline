version: '3'

services:

  streamlit:
    build: 
      context: ../dashboard
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    environment:
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}
    volumes:
      - ../dashboard:/app

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    depends_on:
      - kafka
    ports:
      - "9000:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092


  # Kafka and Zookeeper (inchangés)
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # PostgreSQL for Airflow
  postgres:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  # Construire l'image Airflow personnalisée
  airflow-webserver:
    build:
      context: ../airflow
      dockerfile: Dockerfile
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow/scripts
      - SNOWFLAKE_USER=SURPRIZ13
      - SNOWFLAKE_PASSWORD=Motdepasse13$
      - SNOWFLAKE_ACCOUNT=xfarmgt-lx08599
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/logs:/opt/airflow/logs
      - ../scripts:/opt/airflow/scripts
      - ../dbt_transform:/opt/dbt_transform  # Nouveau volume pour DBT
      - ~/.kaggle:/home/airflow/.kaggle
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    build:
      context: ../airflow
      dockerfile: Dockerfile
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow/scripts
      - SNOWFLAKE_USER=SURPRIZ13
      - SNOWFLAKE_PASSWORD=Motdepasse13$
      - SNOWFLAKE_ACCOUNT=xfarmgt-lx08599
    volumes:
      - ../airflow/dags:/opt/airflow/dags
      - ../airflow/logs:/opt/airflow/logs
      - ../scripts:/opt/airflow/scripts
      - ../dbt_transform:/opt/dbt_transform  # Nouveau volume pour DBT
      - ~/.kaggle:/home/airflow/.kaggle
    command: scheduler

  airflow-init:
    image: apache/airflow:2.7.1
    depends_on:
      - postgres
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=airflow
      - _AIRFLOW_WWW_USER_PASSWORD=airflow
    command: version

  # DBT service (inchangé)
  dbt:
    build: 
      context: ../dbt_transform
      dockerfile: Dockerfile
    volumes:
      - ../dbt_transform/ecommerce:/usr/app/dbt_transform/ecommerce
      - ../dbt_transform/profiles:/root/.dbt
    environment:
      - SNOWFLAKE_USER=${SNOWFLAKE_USER}
      - SNOWFLAKE_PASSWORD=${SNOWFLAKE_PASSWORD}
      - SNOWFLAKE_ACCOUNT=${SNOWFLAKE_ACCOUNT}