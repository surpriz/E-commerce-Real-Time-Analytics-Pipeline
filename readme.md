# ğŸš€ E-commerce Real-Time Analytics Pipeline

![Architecture Overview](diagram-link-placeholder)

Un pipeline de donnÃ©es moderne pour analyser les ventes e-commerce en temps rÃ©el, construit avec Kafka, Snowflake, et DBT.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [PrÃ©requis](#prÃ©requis)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [Structure du projet](#structure-du-projet)
- [DÃ©veloppement](#dÃ©veloppement)
- [Monitoring](#monitoring)
- [Contribution](#contribution)
- [Licence](#licence)

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente un pipeline de donnÃ©es complet pour analyser les ventes e-commerce en temps quasi-rÃ©el. Il combine :
- Ingestion de donnÃ©es en temps rÃ©el avec Kafka
- Stockage et traitement dans Snowflake
- Transformations et modÃ©lisation avec DBT
- Visualisation via des dashboards BI

### FonctionnalitÃ©s principales
- âš¡ Streaming temps rÃ©el des Ã©vÃ©nements de vente
- ğŸ“Š ModÃ©lisation dimensionnelle (Star Schema)
- ğŸ”„ Transformations automatisÃ©es avec DBT
- ğŸ“ˆ Dashboards de monitoring en temps rÃ©el
- ğŸ” Analyses avancÃ©es des performances de vente

## ğŸ— Architecture

```mermaid
graph TD
    A[Sources de donnÃ©es] --> B[Kafka]
    A --> C[Snowflake Stage]
    B --> D[Consumer Stream]
    D --> E[Snowflake RAW]
    C --> E
    E --> F[DBT Transformations]
    F --> G[Snowflake DWH]
    G --> H[Dashboard BI]
```

### Components principaux
- **Sources de donnÃ©es**
  - Ã‰vÃ©nements e-commerce en temps rÃ©el
  - DonnÃ©es historiques (Kaggle datasets)
  - APIs externes (taux de change, mÃ©tÃ©o)
  
- **Pipeline d'ingestion**
  - Kafka pour le streaming temps rÃ©el
  - Batch imports pour les donnÃ©es historiques
  
- **Data Warehouse**
  - Snowflake comme stockage principal
  - ModÃ¨le en Ã©toile optimisÃ©
  
- **Transformations**
  - DBT pour l'orchestration des transformations
  - Tests de qualitÃ© automatisÃ©s
  - Documentation auto-gÃ©nÃ©rÃ©e

## âš™ï¸ PrÃ©requis

- Docker et Docker Compose
- Python 3.9+
- Compte Snowflake
- Compte DBT Cloud (optionnel)
- Un outil BI (Metabase, Tableau, etc.)

## ğŸ›  Installation

1. **Cloner le repo**
```bash
git clone https://github.com/username/ecommerce-analytics
cd ecommerce-analytics
```

2. **CrÃ©er l'environnement virtuel**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
.\venv\Scripts\activate   # Windows
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Lancer l'infrastructure**
```bash
docker-compose up -d
```

## âš¡ Configuration

1. **Variables d'environnement**
CrÃ©er un fichier `.env` Ã  la racine :
```env
# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Snowflake
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database

# API Keys
EXCHANGE_RATE_API_KEY=your_key
```

2. **Configuration DBT**
Modifier `profiles.yml` :
```yaml
ecommerce:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: "{{ env_var('SNOWFLAKE_ACCOUNT') }}"
      user: "{{ env_var('SNOWFLAKE_USER') }}"
      password: "{{ env_var('SNOWFLAKE_PASSWORD') }}"
      warehouse: "{{ env_var('SNOWFLAKE_WAREHOUSE') }}"
      database: "{{ env_var('SNOWFLAKE_DATABASE') }}"
```

## ğŸš€ Utilisation

1. **DÃ©marrer le producteur Kafka**
```bash
python src/producers/order_producer.py
```

2. **Lancer le consumer**
```bash
python src/consumers/snowflake_consumer.py
```

3. **ExÃ©cuter les transformations DBT**
```bash
cd dbt
dbt deps
dbt run
dbt test
```

4. **AccÃ©der aux dashboards**
- Ouvrir votre outil BI
- Connecter Ã  Snowflake avec les credentials fournis
- Importer les dashboards depuis `dashboards/`

## ğŸ“ Structure du projet

```
ecommerce-analytics/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producers/          # Kafka producers
â”‚   â”œâ”€â”€ consumers/          # Kafka consumers
â”‚   â””â”€â”€ utils/              # Utilitaires communs
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/       # Tables de staging
â”‚   â”‚   â”œâ”€â”€ intermediate/  # Tables intermÃ©diaires
â”‚   â”‚   â””â”€â”€ marts/        # Tables finales
â”‚   â”œâ”€â”€ tests/            # Tests personnalisÃ©s
â”‚   â””â”€â”€ macros/           # Macros DBT
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ dashboards/           # Templates BI
â”œâ”€â”€ docs/                 # Documentation
â””â”€â”€ tests/               # Tests unitaires
```

## ğŸ”§ DÃ©veloppement

### Tests
```bash
# Tests unitaires
pytest tests/

# Tests DBT
cd dbt
dbt test

# Linting
black src/
flake8 src/
```

### CI/CD
Le projet utilise GitHub Actions pour :
- ExÃ©cuter les tests automatiquement
- VÃ©rifier le style du code
- DÃ©ployer DBT en production
- GÃ©nÃ©rer la documentation

## ğŸ“Š Monitoring

- **MÃ©triques Kafka** : Prometheus + Grafana
- **Logs** : ELK Stack
- **Alerting** : Configuration dans `monitoring/alerts.yml`

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une nouvelle branche (`git checkout -b feature/amazing-feature`)
3. Commit les changements (`git commit -m 'Add amazing feature'`)
4. Push la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ™‹â€â™‚ï¸ Support

Pour toute question ou problÃ¨me :
- ğŸ“§ Ouvrir une issue
- ğŸ’¬ Contacter l'Ã©quipe Data
- ğŸ“š Consulter la [documentation complÃ¨te](docs/)