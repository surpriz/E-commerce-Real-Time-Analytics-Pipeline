# E-commerce Real-Time Analytics Pipeline

![Dashboard Screenshot](./docs/images/dashboard.png)

Une solution complÃ¨te d'analyse de donnÃ©es e-commerce combinant donnÃ©es historiques et temps rÃ©el, construite avec Kafka, Snowflake, DBT et Streamlit.

## ğŸ“‹ Vue d'ensemble

Ce projet met en place un pipeline de donnÃ©es complet pour analyser les ventes e-commerce selon deux axes :
1. **Analyse historique** : Traitement des donnÃ©es passÃ©es via Kaggle
2. **Analyse temps rÃ©el** : Streaming des nouvelles commandes via Kafka

### Technologies utilisÃ©es

- **Apache Kafka** : Streaming des donnÃ©es temps rÃ©el
- **Snowflake** : Data Warehouse cloud
- **DBT** : Transformation et modÃ©lisation des donnÃ©es
- **Streamlit** : Dashboard de visualisation
- **Airflow** : Orchestration des tÃ¢ches
- **Docker** : Conteneurisation des services

## ğŸ— Architecture

Le pipeline est composÃ© de deux flux principaux :

### 1. Flux Historique
```
Kaggle Dataset -> Python Script -> Snowflake (RAW) -> DBT -> Snowflake (DWH)
```

### 2. Flux Temps RÃ©el
```
Kafka Producer -> Kafka -> Kafka Consumer -> Snowflake (RAW) -> DBT -> Snowflake (DWH)
```

Les deux flux alimentent le mÃªme modÃ¨le de donnÃ©es final, permettant une analyse unifiÃ©e via le dashboard Streamlit.

## âš™ï¸ Installation

1. **PrÃ©requis**
```bash
# Installation des dÃ©pendances systÃ¨me
python 3.9+
docker
docker-compose
```

2. **Configuration**

CrÃ©ez un fichier `.env` Ã  la racine :
```env
SNOWFLAKE_USER=votre_user
SNOWFLAKE_PASSWORD=votre_password
SNOWFLAKE_ACCOUNT=votre_account
```

3. **DÃ©marrage des services**
```bash
# Lancer l'infrastructure
docker-compose up -d

# VÃ©rifier les services
docker-compose ps
```

## ğŸš€ Utilisation

### 1. Ingestion des donnÃ©es historiques

```bash
# Activation de l'environnement virtuel
cd scripts/kafka_scripts
python -m venv venv
source venv/bin/activate

# ExÃ©cution du script d'ingestion
python data_ingestion.py
```

### 2. Flux temps rÃ©el

```bash
# Terminal 1 : Producer
python order_producer.py

# Terminal 2 : Consumer
python order_consumer.py
```

### 3. Transformations DBT

Les transformations DBT crÃ©ent :
- Dimensions (customers, products, sellers)
- Faits (orders)

```bash
# ExÃ©cution des transformations
dbt run
dbt test
```

### 4. Dashboard

Le dashboard est accessible Ã  :
- http://localhost:8501 (Streamlit)
- http://localhost:9000 (Kafdrop - monitoring Kafka)
- http://localhost:8080 (Airflow)

## ğŸ“Š Monitoring

Le projet inclut plusieurs points de monitoring :
- **Kafdrop** : Visualisation des topics et messages Kafka
- **Airflow** : Supervision des tÃ¢ches et pipelines
- **Streamlit** : Dashboard temps rÃ©el avec alertes

## ğŸ“ Structure du projet

```
ecommerce_pipeline/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/                # DAGs Airflow
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                 # DonnÃ©es Kaggle
â”œâ”€â”€ dbt_transform/
â”‚   â”œâ”€â”€ models/             # ModÃ¨les DBT
â”‚   â””â”€â”€ profiles/           # Configuration DBT
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml  # Configuration Docker
â””â”€â”€ scripts/
    â””â”€â”€ kafka_scripts/      # Scripts Python
```

## ğŸ” Points clÃ©s

- **ModÃ¨le de donnÃ©es** : Structure en Ã©toile pour optimiser les analyses
- **Temps rÃ©el** : Latence < 5 secondes pour les nouvelles commandes
- **ScalabilitÃ©** : Architecture distribuÃ©e via Kafka et Snowflake
- **Monitoring** : Alertes sur pics de vente et anomalies
- **Documentation** : Auto-gÃ©nÃ©rÃ©e via DBT

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/amazing-feature`)
3. Commit vos changements (`git commit -m 'Add amazing feature'`)
4. Push vers la branche (`git push origin feature/amazing-feature`)
5. Ouvrir une Pull Request